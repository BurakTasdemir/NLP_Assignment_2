{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEE586 Assignment 2 - Burak Ta≈üdemir 22003996"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import minimize_scalar\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pathlib\n",
    "import pickle\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nnltk.download('punkt')\\nnltk.download('averaged_perceptron_tagger') \\nnltk.download('universal_tagset')\\nnltk.download('wordnet')\\nnltk.download('omw-1.4')\\n\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk extensions need to downloaded if not available\n",
    "\"\"\"\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART A B\n",
    "from nltk.tokenize import word_tokenize\n",
    "with open ('Charles Dickens Processed.txt') as fin:\n",
    "    tokens = word_tokenize(fin.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1898607\n"
     ]
    }
   ],
   "source": [
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART C\n",
    "from nltk import pos_tag\n",
    "pos_tag_tokens = pos_tag(tokens,tagset='universal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('preface', 'NOUN'), ('a', 'DET'), ('chancery', 'NOUN'), ('judge', 'NOUN'), ('once', 'ADV'), ('had', 'VERB'), ('the', 'DET'), ('kindness', 'NOUN'), ('to', 'PRT'), ('inform', 'VERB')]\n"
     ]
    }
   ],
   "source": [
    "print(pos_tag_tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART D\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "lemmatized_tokens=[]\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for w in pos_tag_tokens:\n",
    "    if w[1]=='NOUN':\n",
    "        lemmatized_tokens.append([lemmatizer.lemmatize(w[0], pos='n'),w[1]])\n",
    "    elif w[1]=='ADJ':\n",
    "        lemmatized_tokens.append([lemmatizer.lemmatize(w[0], pos='a'),w[1]])\n",
    "    elif w[1]=='VERB':\n",
    "        lemmatized_tokens.append([lemmatizer.lemmatize(w[0], pos='v'),w[1]])\n",
    "    elif w[1]=='ADV':\n",
    "        lemmatized_tokens.append([lemmatizer.lemmatize(w[0], pos='r'),w[1]])\n",
    "    else:\n",
    "        lemmatized_tokens.append([w[0],w[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21387\n",
      "76792\n",
      "7\n",
      "410\n",
      "68276\n"
     ]
    }
   ],
   "source": [
    "that_count=0\n",
    "the_count=0\n",
    "negligent_count=0\n",
    "london_count=0\n",
    "dot_count=0\n",
    "for w in lemmatized_tokens:\n",
    "    if w[0]=='that':\n",
    "        that_count=that_count+1\n",
    "    if w[0]=='the':\n",
    "        the_count=the_count+1\n",
    "    if w[0]=='negligent':\n",
    "        negligent_count=negligent_count+1\n",
    "    if w[0]=='london':\n",
    "        london_count=london_count+1\n",
    "    if w[0]=='.':\n",
    "        dot_count=dot_count+1\n",
    "print(that_count)\n",
    "print(the_count)\n",
    "print(negligent_count)\n",
    "print(london_count)\n",
    "print(dot_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART E\n",
    "size_one_collocations=[]\n",
    "size_three_collocations=[]\n",
    "for i,w in enumerate(lemmatized_tokens):\n",
    "    if i<len(lemmatized_tokens)-1:\n",
    "        size_one_collocations.append([[w[0],lemmatized_tokens[i+1][0]],[w[1],lemmatized_tokens[i+1][1]]])\n",
    "        size_three_collocations.append([[w[0],lemmatized_tokens[i+1][0]],[w[1],lemmatized_tokens[i+1][1]]])\n",
    "    if i<len(lemmatized_tokens)-2:\n",
    "        size_three_collocations.append([[w[0],lemmatized_tokens[i+2][0]],[w[1],lemmatized_tokens[i+2][1]]])\n",
    "    if i<len(lemmatized_tokens)-3:\n",
    "        size_three_collocations.append([[w[0],lemmatized_tokens[i+3][0]],[w[1],lemmatized_tokens[i+3][1]]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['preface', 'a'], ['NOUN', 'DET']], [['preface', 'chancery'], ['NOUN', 'NOUN']], [['preface', 'judge'], ['NOUN', 'NOUN']], [['a', 'chancery'], ['DET', 'NOUN']], [['a', 'judge'], ['DET', 'NOUN']], [['a', 'once'], ['DET', 'ADV']], [['chancery', 'judge'], ['NOUN', 'NOUN']], [['chancery', 'once'], ['NOUN', 'ADV']], [['chancery', 'have'], ['NOUN', 'VERB']], [['judge', 'once'], ['NOUN', 'ADV']]]\n"
     ]
    }
   ],
   "source": [
    "print(size_three_collocations[0:10][:][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PART F\n",
    "collocation_candidates_one=[]\n",
    "collocation_candidates_three=[]\n",
    "count=0\n",
    "stop_words=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "for i,pair in enumerate(size_one_collocations):\n",
    "    if pair[1]==['NOUN', 'NOUN'] or pair[1]==['ADJ', 'NOUN']:\n",
    "        if (pair[0][0] not in stop_words) and (pair[0][1] not in stop_words):\n",
    "            if (pair[0][0].isalpha()) and (pair[0][1].isalpha()):\n",
    "                collocation_candidates_one.append(pair)\n",
    "        \n",
    "\n",
    "for i,pair in enumerate(size_three_collocations):\n",
    "    if pair[1]==['NOUN', 'NOUN'] or pair[1]==['ADJ', 'NOUN']:\n",
    "        if (pair[0][0] not in stop_words) and (pair[0][1] not in stop_words):\n",
    "            if (pair[0][0].isalpha()) and (pair[0][1].isalpha()):\n",
    "                collocation_candidates_three.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_one=[]\n",
    "pair_values_one=[]\n",
    "word_counts_one=[]\n",
    "for i,pair in enumerate(collocation_candidates_one):\n",
    "    cc_one.append(pair[0][0]+' '+pair[0][1])\n",
    "word_counts_one = collections.Counter(cc_one)\n",
    "for k, v in word_counts_one.items():\n",
    "    if v >=10:\n",
    "        pair_values_one.append([k,v])\n",
    "cc_three=[]\n",
    "pair_values_three=[]\n",
    "word_counts_three=[]\n",
    "for i,pair in enumerate(collocation_candidates_three):\n",
    "    cc_three.append(pair[0][0]+' '+pair[0][1])\n",
    "word_counts_three = collections.Counter(cc_three)\n",
    "for k, v in word_counts_three.items():\n",
    "    if v >=10:\n",
    "        pair_values_three.append([k,v])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['present moment', 24], ['good friend', 44], ['bleak house', 50], ['lord chancellor', 48], ['temple bar', 16], ['old woman', 126], ['young people', 29], ['whole family', 10], ['chancery lane', 23], ['good thing', 32]]\n",
      "[['men woman', 41], ['court chancery', 24], ['present moment', 24], ['jarndyce jarndyce', 57], ['good friend', 48], ['bleak house', 50], ['lord chancellor', 54], ['lincoln inn', 39], ['face earth', 10], ['little boy', 34]]\n"
     ]
    }
   ],
   "source": [
    "print(pair_values_one[0:10])\n",
    "print(pair_values_three[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#answers\n",
    "e1=0\n",
    "for w in size_one_collocations:\n",
    "    if w[0][0]+' '+w[0][1]=='fashionable intelligence':\n",
    "        e1=e1+1\n",
    "print(e1)\n",
    "e1=0\n",
    "for w in size_three_collocations:\n",
    "    if w[0][0]+' '+w[0][1]=='high chancery':\n",
    "        e1=e1+1\n",
    "print(e1)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "e1=0\n",
    "for w in pair_values_one:\n",
    "    if w[0]=='mr. skimpole':\n",
    "        e1=e1+1\n",
    "print(e1)\n",
    "e1=0\n",
    "for w in pair_values_three:\n",
    "    if w[0]=='spontaneous combustion':\n",
    "        e1=e1+1\n",
    "print(e1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(total_token_value,total_token_counts,pair_values,cv):\n",
    "    t_scores=[]\n",
    "    #counter=0\n",
    "    for collocation in pair_values:\n",
    "        y_n='NO'\n",
    "        c_w1w2=collocation[1]\n",
    "        p_w1w2=c_w1w2/total_token_value\n",
    "        [w1,w2]=collocation[0].split()\n",
    "        c_w1=[i[1] for i in total_token_counts if w1 == i[0]]\n",
    "        p_w1=c_w1[0]/total_token_value\n",
    "        c_w2=[i[1] for i in total_token_counts if w2 == i[0]]\n",
    "        if len(c_w2)<1:\n",
    "            c_w2=[1]\n",
    "        p_w2=c_w2[0]/total_token_value\n",
    "        H_0=p_w1*p_w2\n",
    "        t=(p_w1w2-H_0)/(math.sqrt((p_w1w2/total_token_value)))\n",
    "        if t>cv:\n",
    "            y_n='YES'\n",
    "        t_scores.append([collocation[0],t,c_w1w2,c_w1[0],c_w2[0],y_n])   \n",
    "    t_scores.sort(key=lambda x: x[1],reverse=True)\n",
    "    #print(counter)\n",
    "    return t_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(tokens))\n",
    "#print(len(pos_tag_tokens))\n",
    "#print(len(lemmatized_tokens))\n",
    "cv_t=2.576 #critical value for alpha=0.005\n",
    "token_counts=[]\n",
    "total_token_counts=[]\n",
    "total_token_counts_3=[]\n",
    "total_token_value=len(tokens)\n",
    "total_token_value_3=3*len(tokens)\n",
    "token_counts = collections.Counter(tokens)\n",
    "for k, v in token_counts.items():\n",
    "    total_token_counts.append([k,v])\n",
    "    total_token_counts_3.append([k,3*v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_scores_one=t_test(total_token_value,total_token_counts,pair_values_one,cv_t)\n",
    "t_scores_three=t_test(total_token_value_3,total_token_counts_3,pair_values_three,cv_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sir leicester', 19.524567365797857, 383, 3677, 463, 'YES'], ['old gentleman', 19.11954767934326, 371, 3348, 1549, 'YES'], ['young lady', 18.54538447083298, 347, 1777, 1643, 'YES'], ['old lady', 16.918457992235663, 292, 3348, 1643, 'YES'], ['young man', 16.22477189133865, 269, 1777, 3092, 'YES'], ['old man', 15.754679612782613, 259, 3348, 3092, 'YES'], ['miss havisham', 15.010530605988091, 226, 2076, 313, 'YES'], ['long time', 13.26912953637297, 181, 1615, 2918, 'YES'], ['lady dedlock', 13.205730151073704, 175, 1643, 352, 'YES'], ['last night', 12.880177338583197, 168, 1306, 1532, 'YES'], ['dear sir', 12.761353396786513, 172, 2394, 3677, 'YES'], ['bob sawyer', 12.566850317480124, 158, 289, 244, 'YES'], ['young gentleman', 12.211234869542121, 152, 1777, 1549, 'YES'], ['good deal', 12.17031114454712, 149, 2314, 363, 'YES'], ['miss pro', 11.704513074107581, 137, 2076, 2, 'YES'], ['next day', 11.488786992873857, 133, 622, 1541, 'YES'], ['great deal', 11.190991706498865, 126, 1995, 363, 'YES'], ['old woman', 11.08562801733743, 126, 3348, 887, 'YES'], ['fat boy', 11.077288384955802, 123, 220, 1268, 'YES'], ['miss summerson', 11.071015378551426, 123, 2076, 198, 'YES']]\n"
     ]
    }
   ],
   "source": [
    "print(t_scores_one[0:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sir leicester', 19.43293051583172, 383, 11031, 1389, 'YES'], ['old gentleman', 18.915351527984964, 374, 10044, 4647, 'YES'], ['young lady', 18.4345977058228, 349, 5331, 4929, 'YES'], ['old lady', 16.63951275359892, 294, 10044, 4929, 'YES'], ['young man', 15.966007039242024, 272, 5331, 9276, 'YES'], ['old man', 15.17585786795372, 262, 10044, 9276, 'YES'], ['miss havisham', 14.99837245682702, 227, 6228, 939, 'YES'], ['ha ha', 13.391486857963635, 180, 1380, 1380, 'YES'], ['long time', 13.206084141915433, 189, 4845, 8754, 'YES'], ['lady dedlock', 13.197616475993128, 176, 4929, 1056, 'YES'], ['last night', 12.873817746020913, 172, 3918, 4596, 'YES'], ['bob sawyer', 12.5609407724873, 158, 867, 732, 'YES'], ['dear sir', 12.258650866729777, 177, 7182, 11031, 'YES'], ['good deal', 12.097822202173958, 149, 6942, 1089, 'YES'], ['young gentleman', 12.059192498461709, 154, 5331, 4647, 'YES'], ['miss pro', 11.704139400883493, 137, 6228, 6, 'YES'], ['next day', 11.488599654250745, 135, 1866, 4623, 'YES'], ['fat boy', 11.140914760150109, 125, 660, 3804, 'YES'], ['great deal', 11.123030798852943, 126, 5985, 1089, 'YES'], ['miss summerson', 11.12224690016569, 125, 6228, 594, 'YES']]\n"
     ]
    }
   ],
   "source": [
    "print(t_scores_three[0:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_test(total_token_value,total_token_counts,pair_values,cv):\n",
    "    cs_scores=[]\n",
    "    #counter=0\n",
    "    for collocation in pair_values:\n",
    "        y_n='NO'\n",
    "        c_w1w2=collocation[1]\n",
    "        o_11=c_w1w2\n",
    "        [w1,w2]=collocation[0].split()\n",
    "        c_w1=[i[1] for i in total_token_counts if w1 == i[0]]\n",
    "        o_12=(c_w1[0]-c_w1w2)\n",
    "        c_w2=[i[1] for i in total_token_counts if w2 == i[0]]\n",
    "        if len(c_w2)<1:\n",
    "            c_w2=[1]\n",
    "        o_21=(c_w2[0]-c_w1w2)\n",
    "        o_22=total_token_value-o_21-o_12+o_11\n",
    "        chi_score=total_token_value*(o_11*o_22-o_21*o_12)*(o_11*o_22-o_21*o_12)/((o_11+o_12)*(o_11+o_21)*(o_22+o_12)*(o_22+o_21))\n",
    "        if chi_score>cv:\n",
    "            y_n='YES'\n",
    "        cs_scores.append([collocation[0],chi_score,c_w1w2,c_w1[0],c_w2[0],y_n])   \n",
    "    cs_scores.sort(key=lambda x: x[1],reverse=True)\n",
    "    #print(counter)\n",
    "    return cs_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_chi=7.879 #critical value for alpha=0.005\n",
    "chi_scores_one=chi_squared_test(total_token_value,total_token_counts,pair_values_one,cv_chi)\n",
    "chi_scores_three=chi_squared_test(total_token_value_3,total_token_counts_3,pair_values_three,cv_chi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['miss pro', 8591728.106153267, 137, 2076, 2, 'YES'], ['robinson crusoe', 1898607.0, 10, 10, 10, 'YES'], ['south wale', 1477843.3298337294, 12, 37, 5, 'YES'], ['leo hunter', 1467512.657196399, 40, 45, 46, 'YES'], ['chesney wold', 1453598.2353985994, 91, 104, 104, 'YES'], ['dingley dell', 1084908.5715200664, 20, 28, 25, 'YES'], ['nathaniel pipkin', 812080.9226281574, 32, 57, 42, 'YES'], ['saint antoine', 768302.8735329641, 44, 92, 52, 'YES'], ['dame durden', 730996.4921792618, 24, 44, 34, 'YES'], ['gabriel grub', 682669.8853586209, 26, 47, 40, 'YES'], ['bob sawyer', 672015.5506518384, 158, 289, 244, 'YES'], ['horatio fizkin', 656367.0457586384, 11, 14, 25, 'YES'], ['serjeant buzfuz', 609975.7766176012, 41, 109, 48, 'YES'], ['serjeant snubbin', 464948.36533635255, 31, 109, 36, 'YES'], ['john smauker', 342139.70493566676, 26, 121, 31, 'YES'], ['peter magnus', 336578.8867588674, 22, 35, 78, 'YES'], ['stephen blackpool', 324812.2764857532, 32, 171, 35, 'YES'], ['toby crackit', 295662.1150593258, 19, 61, 38, 'YES'], ['bayham badger', 291851.66440317134, 14, 15, 85, 'YES'], ['madame defarge', 275084.00339049706, 92, 194, 301, 'YES']]\n"
     ]
    }
   ],
   "source": [
    "print(chi_scores_one[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['miss pro', 2863726.809600277, 137, 6228, 6, 'YES'], ['robinson crusoe', 632855.6667134848, 10, 30, 30, 'YES'], ['south wale', 492598.4432756398, 12, 111, 15, 'YES'], ['leo hunter', 489117.5534085802, 40, 135, 138, 'YES'], ['chesney wold', 484411.41709117894, 91, 312, 312, 'YES'], ['dingley dell', 361609.5242205925, 20, 84, 75, 'YES'], ['cup saucer', 305568.4915236027, 13, 150, 21, 'YES'], ['quebec malta', 281259.5557023509, 10, 45, 45, 'YES'], ['nathaniel pipkin', 270650.9756643274, 32, 171, 126, 'YES'], ['saint antoine', 256042.29404778001, 44, 276, 156, 'YES'], ['dame durden', 243633.4983483843, 24, 132, 102, 'YES'], ['gabriel grub', 227521.96302864992, 26, 141, 120, 'YES'], ['bob sawyer', 223794.56377490534, 158, 867, 732, 'YES'], ['horatio fizkin', 218774.34881124966, 11, 42, 75, 'YES'], ['knife fork', 207959.33057608735, 37, 255, 147, 'YES'], ['serjeant buzfuz', 203270.59551826344, 41, 327, 144, 'YES'], ['pinch snuff', 199686.75041633614, 26, 126, 153, 'YES'], ['serjeant snubbin', 154941.45772469198, 31, 327, 108, 'YES'], ['dodson fogg', 125492.36684414976, 51, 339, 348, 'YES'], ['lincoln inn', 119305.98210456548, 39, 126, 576, 'YES']]\n"
     ]
    }
   ],
   "source": [
    "print(chi_scores_three[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "def likelihood_test(total_token_value,total_token_counts,pair_values,cv):\n",
    "    like_scores=[]\n",
    "    #counter1=0\n",
    "    #counter2=0\n",
    "    N=total_token_value\n",
    "    for collocation in pair_values:\n",
    "        y_n='NO'\n",
    "        c_w1w2=collocation[1]\n",
    "        c12=c_w1w2\n",
    "        [w1,w2]=collocation[0].split()\n",
    "        c_w1=[i[1] for i in total_token_counts if w1 == i[0]]\n",
    "        c1=(c_w1[0])\n",
    "        c_w2=[i[1] for i in total_token_counts if w2 == i[0]]\n",
    "        if len(c_w2)<1:\n",
    "            c_w2=[1]\n",
    "        c2=(c_w2[0])\n",
    "        p=c2/N\n",
    "        p1=c12/c1\n",
    "        p2=(c2-c12)/(N-c1)\n",
    "        L_H1=binom.pmf(c12, c1, p)*binom.pmf(c2-c12, N-c1, p)\n",
    "        L_H2=binom.pmf(c12, c1, p1)*binom.pmf(c2-c12, N-c1, p2)\n",
    "        if L_H2==0:\n",
    "            L_H2=0.000000001\n",
    "            #counter1=counter1+1\n",
    "        if L_H1==0:\n",
    "            L_H1=0.000000001\n",
    "            #counter2=counter2+1\n",
    "        like_score=math.log(L_H1/L_H2)\n",
    "        if (-2)*like_score>cv:\n",
    "            y_n='YES'\n",
    "        like_scores.append([collocation[0],(-2)*like_score,c_w1w2,c_w1[0],c_w2[0],y_n])   \n",
    "    like_scores.sort(key=lambda x: x[1],reverse=True)\n",
    "    #print(counter1,counter2)\n",
    "    return like_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_likelihood=7.879 #critical value for alpha=0.005\n",
    "likelihood_scores_one=likelihood_test(total_token_value,total_token_counts,pair_values_one,cv_likelihood)\n",
    "likelihood_scores_three=likelihood_test(total_token_value_3,total_token_counts_3,pair_values_three,cv_likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['miss summerson', 1422.1836274554996, 123, 2076, 198, 'YES'], ['last night', 1411.39764275092, 168, 1306, 1532, 'YES'], ['leicester dedlock', 1328.4552018349252, 104, 463, 352, 'YES'], ['master copperfield', 1323.688516181626, 113, 543, 523, 'YES'], ['great deal', 1268.130940694088, 126, 1995, 363, 'YES'], ['next day', 1259.9006205867483, 133, 622, 1541, 'YES'], ['long time', 1227.43484074038, 181, 1615, 2918, 'YES'], ['young gentleman', 1141.5484704396054, 152, 1777, 1549, 'YES'], ['many year', 1131.4516390272656, 99, 1206, 249, 'YES'], ['young woman', 980.3080999674092, 120, 1777, 887, 'YES'], ['low voice', 950.5224954377943, 89, 464, 758, 'YES'], ['dear sir', 928.1022974771436, 172, 2394, 3677, 'YES'], ['old woman', 880.2071593159516, 126, 3348, 887, 'YES'], ['miss murdstone', 782.4560601825631, 83, 2076, 299, 'YES'], ['lord chancellor', 720.9863978639651, 48, 296, 93, 'YES'], ['miss flite', 699.0585542708438, 60, 2076, 93, 'YES'], ['masr davy', 698.6835002800832, 45, 115, 169, 'YES'], ['miss mill', 696.7420506707078, 51, 2076, 51, 'YES'], ['young friend', 684.2869847056392, 94, 1777, 1053, 'YES'], ['little woman', 671.5107057424699, 107, 4004, 887, 'YES']]\n"
     ]
    }
   ],
   "source": [
    "print(likelihood_scores_one[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chesney wold', 1438.7542584601756, 91, 312, 312, 'YES'], ['young man', 1368.021118162014, 272, 5331, 9276, 'YES'], ['fat boy', 1192.047909300397, 125, 660, 3804, 'YES'], ['good deal', 1135.6538890687625, 149, 6942, 1089, 'YES'], ['leicester dedlock', 1133.8229387390452, 110, 1389, 1056, 'YES'], ['madame defarge', 1111.713470485225, 92, 582, 903, 'YES'], ['miss summerson', 1096.8680933897306, 125, 6228, 594, 'YES'], ['many year', 1078.758286475786, 117, 3618, 747, 'YES'], ['master copperfield', 1051.9899816415034, 114, 1629, 1569, 'YES'], ['last night', 1050.7828644327863, 172, 3918, 4596, 'YES'], ['old man', 974.7710351877913, 262, 10044, 9276, 'YES'], ['next day', 959.0637157185304, 135, 1866, 4623, 'YES'], ['low voice', 958.6571320340312, 110, 1392, 2274, 'YES'], ['great deal', 952.6044259754885, 126, 5985, 1089, 'YES'], ['long time', 870.0542330576999, 189, 4845, 8754, 'YES'], ['dedlock baronet', 858.1712481876118, 66, 1056, 237, 'YES'], ['young gentleman', 808.4362769763383, 154, 5331, 4647, 'YES'], ['leicester baronet', 791.410156480708, 64, 1389, 237, 'YES'], ['doctor manette', 770.8396606777319, 75, 2085, 489, 'YES'], ['young woman', 710.7565724954762, 121, 5331, 2661, 'YES']]\n"
     ]
    }
   ],
   "source": [
    "print(likelihood_scores_three[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams=['cursitor street','good one']\n",
    "cursitor_street_scores=[]\n",
    "good_one_scores=[]\n",
    "for w in t_scores_one:\n",
    "    if w[0]==bigrams[0]:\n",
    "        cursitor_street_scores.append(w)\n",
    "    if w[0]==bigrams[1]:\n",
    "        good_one_scores.append(w)\n",
    "for w in chi_scores_one:\n",
    "    if w[0]==bigrams[0]:\n",
    "        cursitor_street_scores.append(w)\n",
    "    if w[0]==bigrams[1]:\n",
    "        good_one_scores.append(w)\n",
    "for w in likelihood_scores_one:\n",
    "    if w[0]==bigrams[0]:\n",
    "        cursitor_street_scores.append(w)\n",
    "    if w[0]==bigrams[1]:\n",
    "        good_one_scores.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cursitor street', 3.7405213969549673, 14, 15, 538, 'YES'], ['cursitor street', 46097.81920167476, 14, 15, 538, 'YES'], ['cursitor street', 221.7456909465461, 14, 15, 538, 'YES']]\n"
     ]
    }
   ],
   "source": [
    "print(cursitor_street_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['good one', 1.5092340045489974, 10, 2314, 4289, 'NO'], ['good one', 4.372709977302959, 10, 2314, 4289, 'NO'], ['good one', 3.44345626726946, 10, 2314, 4289, 'NO']]\n"
     ]
    }
   ],
   "source": [
    "print(good_one_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
